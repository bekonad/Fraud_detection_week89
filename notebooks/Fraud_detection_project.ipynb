{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7cTrjkdIiYu",
        "outputId": "fbddc5c3-09b7-47e9-e5c1-be010dc46d47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['# Fraud Detection Project - End-to-End Pipeline\\n',\n",
              "    '\\n',\n",
              "    '## Overview\\n',\n",
              "    'This notebook executes the end-to-end pipeline for my 10 Academy KAIM Week 8 & 9 Challenge submission, combining Tasks 1â€“3. Last updated: 02:45 AM EAT, Tuesday, August 26, 2025.
              "    '\\n',\n",
              "    '## Objectives\\n',\n",
              "    '- Run data preprocessing, EDA, model training, evaluation, and SHAP analysis.\\n',\n",
              "    '- Generate all deliverables.']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 1,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Install all required packages\\n',\n",
              "    '!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib shap xgboost selenium\\n',\n",
              "    '\\n',\n",
              "    '# Import libraries\\n',\n",
              "    'import pandas as pd\\n',\n",
              "    'import numpy as np\\n',\n",
              "    'import matplotlib.pyplot as plt\\n',\n",
              "    'import seaborn as sns\\n',\n",
              "    'from imblearn.over_sampling import SMOTE\\n',\n",
              "    'from sklearn.model_selection import train_test_split\\n',\n",
              "    'from sklearn.linear_model import LogisticRegression\\n',\n",
              "    'from sklearn.ensemble import RandomForestClassifier\\n',\n",
              "    'from xgboost import XGBClassifier\\n',\n",
              "    'from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix\\n',\n",
              "    'import joblib\\n',\n",
              "    'import shap\\n',\n",
              "    'import logging\\n',\n",
              "    'import os\\n',\n",
              "    '\\n',\n",
              "    '# Configure logging\\n',\n",
              "    \"logging.basicConfig(level=logging.INFO, filename='/content/drive/MyDrive/outputs/project_output.log', format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
              "    'logger = logging.getLogger(__name__)\\n',\n",
              "    '\\n',\n",
              "    '# Mount Google Drive\\n',\n",
              "    'from google.colab import drive\\n',\n",
              "    \"drive.mount('/content/drive')\\n\",\n",
              "    '\\n',\n",
              "    '# Create necessary folders\\n',\n",
              "    'folders = [\\n',\n",
              "    \"    '/content/drive/MyDrive/Data/raw',\\n\",\n",
              "    \"    '/content/drive/MyDrive/Data/processed',\\n\",\n",
              "    \"    '/content/drive/MyDrive/reports/eda',\\n\",\n",
              "    \"    '/content/drive/MyDrive/reports/confusion_matrices',\\n\",\n",
              "    \"    '/content/drive/MyDrive/reports/shap',\\n\",\n",
              "    \"    '/content/drive/MyDrive/reports/models',\\n\",\n",
              "    \"    '/content/drive/MyDrive/outputs',\\n\",\n",
              "    \"    '/content/drive/MyDrive/scripts',\\n\",\n",
              "    \"    '/content/drive/MyDrive/analysis'\\n\",\n",
              "    \"    '/content/drive/MyDrive/notebooks'\\n\",\n",
              "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task1.ipynb'\\n\",\n",
              "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task2.ipynb'\\n\",\n",
              "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task3.ipynb'\\n\",\n",
              "    \"    '/content/drive/MyDrive/outputs/fraud_detection_project.ipynb'\\n\",\n",
              "    ']\\n',\n",
              "    'for folder in folders:\\n',\n",
              "    '    os.makedirs(folder, exist_ok=True)\\n',\n",
              "    '    logger.info(f\"Created folder: {folder}\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Task 1: Data Preprocessing and EDA']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 1,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Load and preprocess data (from task1.ipynb)\\n',\n",
              "    \"fraud_data = pd.read_csv('/content/drive/MyDrive/Data/raw/Fraud_Data.csv')\\n\",\n",
              "    \"creditcard_data = pd.read_csv('/content/drive/MyDrive/Data/raw/creditcard.csv')\\n\",\n",
              "    \"ip_to_country = pd.read_csv('/content/drive/MyDrive/Data/raw/IpAddress_to_Country.csv')\\n\",\n",
              "    '\\n',\n",
              "    'fraud_data = fraud_data.drop_duplicates().dropna()\\n',\n",
              "    \"fraud_data['ip_address'] = fraud_data['ip_address'].astype(float).astype(int)\\n\",\n",
              "    \"fraud_data['country'] = fraud_data['ip_address'].apply(lambda x: ip_to_country[ip_to_country['ip_start'] <= x].iloc[-1]['country'] if not ip_to_country.empty else 'Unknown')\\n\",\n",
              "    '\\n',\n",
              "    \"fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\\n\",\n",
              "    \"fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\\n\",\n",
              "    \"fraud_data['time_to_purchase'] = (fraud_data['purchase_time'] - fraud_data['signup_time']).dt.total_seconds() / 3600\\n\",\n",
              "    '\\n',\n",
              "    \"features = ['time_to_purchase', 'purchase_value', 'country']\\n\",\n",
              "    'X = fraud_data[features]\\n',\n",
              "    \"y = fraud_data['is_fraud']\\n\",\n",
              "    '\\n',\n",
              "    '# EDA visualizations\\n',\n",
              "    'plt.figure(figsize=(8, 6))\\n',\n",
              "    \"sns.countplot(x='is_fraud', data=fraud_data)\\n\",\n",
              "    \"plt.title('Class Distribution')\\n\",\n",
              "    \"plt.savefig('/content/drive/MyDrive/reports/eda/class_distribution.png')\\n\",\n",
              "    'plt.close()\\n',\n",
              "    '\\n',\n",
              "    'plt.figure(figsize=(8, 6))\\n',\n",
              "    \"sns.histplot(data=fraud_data, x='purchase_value', hue='is_fraud', kde=True)\\n\",\n",
              "    \"plt.title('Purchase Value Distribution by Fraud Status')\\n\",\n",
              "    \"plt.savefig('/content/drive/MyDrive/reports/eda/purchase_patterns.png')\\n\",\n",
              "    'plt.close()\\n',\n",
              "    '\\n',\n",
              "    'plt.figure(figsize=(10, 6))\\n',\n",
              "    \"sns.boxplot(x='is_fraud', y='purchase_value', data=fraud_data)\\n\",\n",
              "    \"plt.title('Purchase Value vs Fraud Status')\\n\",\n",
              "    \"plt.savefig('/content/drive/MyDrive/reports/eda/bivariate_boxplots.png')\\n\",\n",
              "    'plt.close()\\n',\n",
              "    '\\n',\n",
              "    '# Apply SMOTE and save\\n',\n",
              "    'smote = SMOTE(random_state=42)\\n',\n",
              "    'X_resampled, y_resampled = smote.fit_resample(X, y)\\n',\n",
              "    'X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\\n',\n",
              "    '\\n',\n",
              "    'train_data = pd.concat([X_train, y_train], axis=1)\\n',\n",
              "    'test_data = pd.concat([X_test, y_test], axis=1)\\n',\n",
              "    \"train_data.to_csv('/content/drive/MyDrive/Data/processed/fraud_train_processed.csv', index=False)\\n\",\n",
              "    \"test_data.to_csv('/content/drive/MyDrive/Data/processed/fraud_test_processed.csv', index=False)\\n\",\n",
              "    \"y_train.to_csv('/content/drive/MyDrive/Data/processed/y_fraud_train.csv', index=False)\\n\",\n",
              "    \"y_test.to_csv('/content/drive/MyDrive/Data/processed/y_fraud_test.csv', index=False)\\n\",\n",
              "    'logger.info(\"Completed Task 1: Data preprocessing and EDA.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Task 2: Model Training and Evaluation']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 1,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Train and evaluate models (from task2.ipynb)\\n',\n",
              "    'models = {\\n',\n",
              "    \"    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\\n\",\n",
              "    \"    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\\n\",\n",
              "    \"    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\\n\",\n",
              "    '}\\n',\n",
              "    '\\n',\n",
              "    'results = {}\\n',\n",
              "    'for name, model in models.items():\\n',\n",
              "    '    model.fit(X_train, y_train.values.ravel())\\n',\n",
              "    '    y_pred_proba = model.predict_proba(X_test)[:, 1]\\n',\n",
              "    '    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\\n',\n",
              "    '    f1 = f1_score(y_test, (y_pred_proba > 0.5).astype(int))\\n',\n",
              "    \"    results[name] = {'F1-Score': f1, 'AUC-PR': np.trapz(recall, precision)}\\n\",\n",
              "    '    logger.info(f\"{name} trained and evaluated with F1-Score: {f1:.4f}, AUC-PR: {results[name][\\'AUC-PR\\']:.4f}\")\\n',\n",
              "    '\\n',\n",
              "    'for name, model in models.items():\\n',\n",
              "    '    joblib.dump(model, f\\'/content/drive/MyDrive/reports/models/{name.lower().replace(\" \", \"_\")}_model.pkl\\')\\n',\n",
              "    '\\n',\n",
              "    \"with open('/content/drive/MyDrive/reports/model_results.txt', 'w') as f:\\n\",\n",
              "    '    for name, metrics in results.items():\\n',\n",
              "    '        f.write(f\"{name}: F1-Score = {metrics[\\'F1-Score\\']:.4f}, AUC-PR = {metrics[\\'AUC-PR\\']:.4f}\\\\n\")\\n',\n",
              "    '\\n',\n",
              "    '# Confusion matrices\\n',\n",
              "    'for name, model in models.items():\\n',\n",
              "    '    y_pred = model.predict(X_test)\\n',\n",
              "    '    cm = confusion_matrix(y_test, y_pred)\\n',\n",
              "    '    plt.figure(figsize=(6, 6))\\n',\n",
              "    \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\\n\",\n",
              "    \"    plt.title(f'Confusion Matrix - {name}')\\n\",\n",
              "    '    plt.savefig(f\\'/content/drive/MyDrive/reports/confusion_matrices/confusion_matrix_{name.lower().replace(\" \", \"_\")}.png\\')\\n',\n",
              "    '    plt.close()\\n',\n",
              "    'logger.info(\"Completed Task 2: Model training and evaluation.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Task 3: SHAP Analysis and Report Generation']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': 1,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# SHAP analysis and report generation (from task3.ipynb)\\n',\n",
              "    \"xgb_model = joblib.load('/content/drive/MyDrive/reports/models/xgboost_model.pkl')\\n\",\n",
              "    'explainer = shap.TreeExplainer(xgb_model)\\n',\n",
              "    'shap_values = explainer.shap_values(X_test)\\n',\n",
              "    '\\n',\n",
              "    'shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\\n',\n",
              "    \"plt.savefig('/content/drive/MyDrive/reports/shap/shap_summary.png')\\n\",\n",
              "    'plt.close()\\n',\n",
              "    '\\n',\n",
              "    'shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], matplotlib=True)\\n',\n",
              "    \"plt.savefig('/content/drive/MyDrive/reports/shap/shap_force_plot.png')\\n\",\n",
              "    'plt.close()\\n',\n",
              "    '\\n',\n",
              "    '!latexmk -pdf /content/drive/MyDrive/scripts/report.tex -outdir=/content/drive/MyDrive/analysis\\n',\n",
              "    '!latexmk -pdf /content/drive/MyDrive/scripts/blog_post.tex -outdir=/content/drive/MyDrive/analysis\\n',\n",
              "    'logger.info(\"Completed Task 3: SHAP analysis and report generation.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Conclusion\\n',\n",
              "    'The end-to-end pipeline is complete. All deliverables are saved in `Data/processed/`, `reports/`, and `analysis/`. Logs are in `outputs/project_output.log`.']}],\n",
              " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
              "   'language': 'python',\n",
              "   'name': 'python3'},\n",
              "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
              "   'file_extension': '.py',\n",
              "   'mimetype': 'text/x-python',\n",
              "   'name': 'python',\n",
              "   'nbconvert_exporter': 'python',\n",
              "   'pygments_lexer': 'ipython3',\n",
              "   'version': '3.10'}},\n",
              " 'nbformat': 4,\n",
              " 'nbformat_minor': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# Fraud Detection Project - End-to-End Pipeline\\n\",\n",
        "    \"\\n\",\n",
        "    \"## Overview\\n\",\n",
        "    \"This notebook executes the end-to-end pipeline for my 10 Academy KAIM Week 8 & 9 Challenge submission, combining Tasks 1â€“3. Last updated: 02:45 AM EAT, Tuesday, August 26, 2025.
        "    \"\\n\",\n",
        "    \"## Objectives\\n\",\n",
        "    \"- Run data preprocessing, EDA, model training, evaluation, and SHAP analysis.\\n\",\n",
        "    \"- Generate all deliverables.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 1,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Install all required packages\\n\",\n",
        "    \"!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib shap xgboost selenium\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Import libraries\\n\",\n",
        "    \"import pandas as pd\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import matplotlib.pyplot as plt\\n\",\n",
        "    \"import seaborn as sns\\n\",\n",
        "    \"from imblearn.over_sampling import SMOTE\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
        "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
        "    \"from xgboost import XGBClassifier\\n\",\n",
        "    \"from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix\\n\",\n",
        "    \"import joblib\\n\",\n",
        "    \"import shap\\n\",\n",
        "    \"import logging\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Configure logging\\n\",\n",
        "    \"logging.basicConfig(level=logging.INFO, filename='/content/drive/MyDrive/outputs/project_output.log', format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
        "    \"logger = logging.getLogger(__name__)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Mount Google Drive\\n\",\n",
        "    \"from google.colab import drive\\n\",\n",
        "    \"drive.mount('/content/drive')\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Create necessary folders\\n\",\n",
        "    \"folders = [\\n\",\n",
        "    \"    '/content/drive/MyDrive/Data/raw',\\n\",\n",
        "    \"    '/content/drive/MyDrive/Data/processed',\\n\",\n",
        "    \"    '/content/drive/MyDrive/reports/eda',\\n\",\n",
        "    \"    '/content/drive/MyDrive/reports/confusion_matrices',\\n\",\n",
        "    \"    '/content/drive/MyDrive/reports/shap',\\n\",\n",
        "    \"    '/content/drive/MyDrive/reports/models',\\n\",\n",
        "    \"    '/content/drive/MyDrive/outputs',\\n\",\n",
        "    \"    '/content/drive/MyDrive/scripts',\\n\",\n",
        "    \"    '/content/drive/MyDrive/analysis'\\n\",\n",
        "    \"    '/content/drive/MyDrive/notebooks'\\n\",\n",
        "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task1.ipynb'\\n\",\n",
        "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task2.ipynb'\\n\",\n",
        "    \"    '/content/drive/MyDrive/notebooks/fraud_detection_task3.ipynb'\\n\",\n",
        "    \"    '/content/drive/MyDrive/outputs/fraud_detection_project.ipynb'\\n\",\n",
        "    \"]\\n\",\n",
        "    \"for folder in folders:\\n\",\n",
        "    \"    os.makedirs(folder, exist_ok=True)\\n\",\n",
        "    \"    logger.info(f\\\"Created folder: {folder}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Task 1: Data Preprocessing and EDA\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 1,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Load and preprocess data (from task1.ipynb)\\n\",\n",
        "    \"fraud_data = pd.read_csv('/content/drive/MyDrive/Data/raw/Fraud_Data.csv')\\n\",\n",
        "    \"creditcard_data = pd.read_csv('/content/drive/MyDrive/Data/raw/creditcard.csv')\\n\",\n",
        "    \"ip_to_country = pd.read_csv('/content/drive/MyDrive/Data/raw/IpAddress_to_Country.csv')\\n\",\n",
        "    \"\\n\",\n",
        "    \"fraud_data = fraud_data.drop_duplicates().dropna()\\n\",\n",
        "    \"fraud_data['ip_address'] = fraud_data['ip_address'].astype(float).astype(int)\\n\",\n",
        "    \"fraud_data['country'] = fraud_data['ip_address'].apply(lambda x: ip_to_country[ip_to_country['ip_start'] <= x].iloc[-1]['country'] if not ip_to_country.empty else 'Unknown')\\n\",\n",
        "    \"\\n\",\n",
        "    \"fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\\n\",\n",
        "    \"fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\\n\",\n",
        "    \"fraud_data['time_to_purchase'] = (fraud_data['purchase_time'] - fraud_data['signup_time']).dt.total_seconds() / 3600\\n\",\n",
        "    \"\\n\",\n",
        "    \"features = ['time_to_purchase', 'purchase_value', 'country']\\n\",\n",
        "    \"X = fraud_data[features]\\n\",\n",
        "    \"y = fraud_data['is_fraud']\\n\",\n",
        "    \"\\n\",\n",
        "    \"# EDA visualizations\\n\",\n",
        "    \"plt.figure(figsize=(8, 6))\\n\",\n",
        "    \"sns.countplot(x='is_fraud', data=fraud_data)\\n\",\n",
        "    \"plt.title('Class Distribution')\\n\",\n",
        "    \"plt.savefig('/content/drive/MyDrive/reports/eda/class_distribution.png')\\n\",\n",
        "    \"plt.close()\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.figure(figsize=(8, 6))\\n\",\n",
        "    \"sns.histplot(data=fraud_data, x='purchase_value', hue='is_fraud', kde=True)\\n\",\n",
        "    \"plt.title('Purchase Value Distribution by Fraud Status')\\n\",\n",
        "    \"plt.savefig('/content/drive/MyDrive/reports/eda/purchase_patterns.png')\\n\",\n",
        "    \"plt.close()\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.figure(figsize=(10, 6))\\n\",\n",
        "    \"sns.boxplot(x='is_fraud', y='purchase_value', data=fraud_data)\\n\",\n",
        "    \"plt.title('Purchase Value vs Fraud Status')\\n\",\n",
        "    \"plt.savefig('/content/drive/MyDrive/reports/eda/bivariate_boxplots.png')\\n\",\n",
        "    \"plt.close()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Apply SMOTE and save\\n\",\n",
        "    \"smote = SMOTE(random_state=42)\\n\",\n",
        "    \"X_resampled, y_resampled = smote.fit_resample(X, y)\\n\",\n",
        "    \"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\\n\",\n",
        "    \"\\n\",\n",
        "    \"train_data = pd.concat([X_train, y_train], axis=1)\\n\",\n",
        "    \"test_data = pd.concat([X_test, y_test], axis=1)\\n\",\n",
        "    \"train_data.to_csv('/content/drive/MyDrive/Data/processed/fraud_train_processed.csv', index=False)\\n\",\n",
        "    \"test_data.to_csv('/content/drive/MyDrive/Data/processed/fraud_test_processed.csv', index=False)\\n\",\n",
        "    \"y_train.to_csv('/content/drive/MyDrive/Data/processed/y_fraud_train.csv', index=False)\\n\",\n",
        "    \"y_test.to_csv('/content/drive/MyDrive/Data/processed/y_fraud_test.csv', index=False)\\n\",\n",
        "    \"logger.info(\\\"Completed Task 1: Data preprocessing and EDA.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Task 2: Model Training and Evaluation\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 1,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Train and evaluate models (from task2.ipynb)\\n\",\n",
        "    \"models = {\\n\",\n",
        "    \"    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\\n\",\n",
        "    \"    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\\n\",\n",
        "    \"    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\\n\",\n",
        "    \"}\\n\",\n",
        "    \"\\n\",\n",
        "    \"results = {}\\n\",\n",
        "    \"for name, model in models.items():\\n\",\n",
        "    \"    model.fit(X_train, y_train.values.ravel())\\n\",\n",
        "    \"    y_pred_proba = model.predict_proba(X_test)[:, 1]\\n\",\n",
        "    \"    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\\n\",\n",
        "    \"    f1 = f1_score(y_test, (y_pred_proba > 0.5).astype(int))\\n\",\n",
        "    \"    results[name] = {'F1-Score': f1, 'AUC-PR': np.trapz(recall, precision)}\\n\",\n",
        "    \"    logger.info(f\\\"{name} trained and evaluated with F1-Score: {f1:.4f}, AUC-PR: {results[name]['AUC-PR']:.4f}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"for name, model in models.items():\\n\",\n",
        "    \"    joblib.dump(model, f'/content/drive/MyDrive/reports/models/{name.lower().replace(\\\" \\\", \\\"_\\\")}_model.pkl')\\n\",\n",
        "    \"\\n\",\n",
        "    \"with open('/content/drive/MyDrive/reports/model_results.txt', 'w') as f:\\n\",\n",
        "    \"    for name, metrics in results.items():\\n\",\n",
        "    \"        f.write(f\\\"{name}: F1-Score = {metrics['F1-Score']:.4f}, AUC-PR = {metrics['AUC-PR']:.4f}\\\\n\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Confusion matrices\\n\",\n",
        "    \"for name, model in models.items():\\n\",\n",
        "    \"    y_pred = model.predict(X_test)\\n\",\n",
        "    \"    cm = confusion_matrix(y_test, y_pred)\\n\",\n",
        "    \"    plt.figure(figsize=(6, 6))\\n\",\n",
        "    \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\\n\",\n",
        "    \"    plt.title(f'Confusion Matrix - {name}')\\n\",\n",
        "    \"    plt.savefig(f'/content/drive/MyDrive/reports/confusion_matrices/confusion_matrix_{name.lower().replace(\\\" \\\", \\\"_\\\")}.png')\\n\",\n",
        "    \"    plt.close()\\n\",\n",
        "    \"logger.info(\\\"Completed Task 2: Model training and evaluation.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Task 3: SHAP Analysis and Report Generation\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 1,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# SHAP analysis and report generation (from task3.ipynb)\\n\",\n",
        "    \"xgb_model = joblib.load('/content/drive/MyDrive/reports/models/xgboost_model.pkl')\\n\",\n",
        "    \"explainer = shap.TreeExplainer(xgb_model)\\n\",\n",
        "    \"shap_values = explainer.shap_values(X_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"shap.summary_plot(shap_values, X_test, plot_type=\\\"bar\\\")\\n\",\n",
        "    \"plt.savefig('/content/drive/MyDrive/reports/shap/shap_summary.png')\\n\",\n",
        "    \"plt.close()\\n\",\n",
        "    \"\\n\",\n",
        "    \"shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], matplotlib=True)\\n\",\n",
        "    \"plt.savefig('/content/drive/MyDrive/reports/shap/shap_force_plot.png')\\n\",\n",
        "    \"plt.close()\\n\",\n",
        "    \"\\n\",\n",
        "    \"!latexmk -pdf /content/drive/MyDrive/scripts/report.tex -outdir=/content/drive/MyDrive/analysis\\n\",\n",
        "    \"!latexmk -pdf /content/drive/MyDrive/scripts/blog_post.tex -outdir=/content/drive/MyDrive/analysis\\n\",\n",
        "    \"logger.info(\\\"Completed Task 3: SHAP analysis and report generation.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Conclusion\\n\",\n",
        "    \"The end-to-end pipeline is complete. All deliverables are saved in `Data/processed/`, `reports/`, and `analysis/`. Logs are in `outputs/project_output.log`.\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.10\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrdDQ4eCkuX7",
        "outputId": "d55485ef-cdb3-450d-d98a-43c2bfb7ddd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bekonad/Fraud_detection_week89.git\n",
        "%cd Fraud_detection_week89\n",
        "!git lfs install\n",
        "!git lfs pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTOkudJcT0zR",
        "outputId": "4d9d58bb-8f0f-4408-ee4a-b2bae4ec0af0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Fraud_detection_week89'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 194 (delta 51), reused 160 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (194/194), 10.12 MiB | 17.27 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Filtering content: 100% (30/30), 393.21 MiB | 60.11 MiB/s, done.\n",
            "/content/Fraud_detection_week89\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"bereketfeleke003@gmail.com\"\n",
        "!git config --global user.name \"bekonad\""
      ],
      "metadata": {
        "id": "kMRnwGbxWJXy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_v1Bg4lWNvc",
        "outputId": "b014e2ee-6f37-4e91-b18f-1307c83acd92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4V30kjDYsE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}